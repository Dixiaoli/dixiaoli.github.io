{"meta":{"title":"Hexo","subtitle":"","description":"","author":"十三先森","url":"http://dixiaoli.github.io","root":"/"},"pages":[],"posts":[{"title":"Spark-local & stand-alone配置","slug":"Spark-local-stand-alone配置","date":"2022-05-20T07:46:11.000Z","updated":"2022-05-28T08:54:23.775Z","comments":true,"path":"2022/05/20/Spark-local-stand-alone配置/","link":"","permalink":"http://dixiaoli.github.io/2022/05/20/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Spark安装配置 Spark是专为大规模数据处理而设计的快速通用的计算引擎，其提供了一个全面、统一的框架用于管理各种不同性质的数据集和数据源的大数据处理的需求，大数据开发需掌握Spark基础、SparkJob、Spark RDD、spark job部署与资源分配、Spark shuffle、Spark内存管理、Spark广播变量、Spark SQL、Spark Streaming以及Spark ML等相关知识。 一、Spark-local模式本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境 Anaconda On Linux 安装 (单台服务器脚本安装) 安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server: 1234cd /export/server# 运行文件sh Anaconda3-2021.05-Linux-x86_64.sh 123456789101112过程显示：...# 出现内容选 yesPlease answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;&gt;&gt;&gt; yes...# 出现添加路径：/export/server/anaconda3...[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3PREFIX=/export/server/anaconda3... 安装完成后, 退出终端， 重新进来: 1exit 1234结果显示：# 看到这个Base开头表明安装好了.base是默认的虚拟环境.Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1(base) [root@master ~]# 创建虚拟环境 pyspark 基于 python3.8 1conda create -n pyspark python=3.8 切换到虚拟环境内 1conda activate pyspark 123结果显示：(base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]# 在虚拟环境内安装包 （有WARNING不用管） 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple spark 安装 将文件上传到 &#x2F;export&#x2F;server 里面 ，解压 1234cd /export/server# 解压tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ 建立软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 添加环境变量 SPARK_HOME: 表示Spark安装路径在哪里PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器JAVA_HOME: 告知Spark Java在哪里HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里HADOOP_HOME: 告知Spark Hadoop安装在哪里 vim /etc/profile 内容： ..... 注：此部分之前配置过，此部分不需要在配置 #JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar #HADOOP_HOME export HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin #ZOOKEEPER_HOME export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin ..... # 将以下部分添加进去 #SPARK_HOME export SPARK_HOME=/export/server/spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 123456789- ```shell vim .bashrc 内容添加进去： #JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 重新加载环境变量文件 12source /etc/profilesource ~/.bashrc 进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹 1cd /export/server/anaconda3/envs/pyspark/bin/ 开启 1./pyspark 1234567891011121314151617181920结果显示：(base) [root@master bin]# ./pysparkPython 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Setting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &#x27;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.2.0 /_/Using Python version 3.8.12 (default, Oct 12 2021 13:49:34)Spark context Web UI available at http://master:4040Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1647347826262).SparkSession available as &#x27;spark&#x27;.&gt;&gt;&gt; 查看WebUI界面 123浏览器访问：http://node1:4040/ 退出 1conda deactivate 二、Spark-Standalone模式Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在node2 和 node3 上部署) 安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server: 1234cd /export/server# 运行文件sh Anaconda3-2021.05-Linux-x86_64.sh 123456789101112过程显示：...# 出现内容选 yesPlease answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;&gt;&gt;&gt; yes...# 出现添加路径：/export/server/anaconda3...[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3PREFIX=/export/server/anaconda3... 安装完成后, 退出终端， 重新进来: 1exit 1234结果显示：# 看到这个Base开头表明安装好了.base是默认的虚拟环境.Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1(base) [root@node1 ~]# 在 master 节点上把 .&#x2F;bashrc 和 profile 分发给 node2 和 node3 1234567#分发 .bashrc :scp ~/.bashrc root@node2:~/scp ~/.bashrc root@node3:~/#分发 profile :scp /etc/profile/ root@node2:/etc/scp /etc/profile/ root@node3:/etc/ 创建虚拟环境 pyspark 基于 python3.8 1conda create -n pyspark python=3.8 切换到虚拟环境内 1conda activate pyspark 123结果显示：(base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]# 在虚拟环境内安装包 （有WARNING不用管） 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple node1 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件 1cd /export/server/spark/conf 将文件 workers.template 改名为 workers，并配置文件内容 123456789mv workers.template workersvim workers# localhost删除，内容追加文末：node1node2node3# 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker 将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容 123456789101112131415161718192021222324252627282930313233mv spark-env.sh.template spark-env.shvim spark-env.sh文末追加内容：## 设置JAVA安装目录JAVA_HOME=/export/server/jdk## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoopYARN_CONF_DIR=/export/server/hadoop/etc/hadoop## 指定spark Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上export SPARK_MASTER_HOST=node# 告知sparkmaster的通讯端口export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口SPARK_MASTER_WEBUI_PORT=8080# worker cpu可用核数SPARK_WORKER_CORES=1# worker可用内存SPARK_WORKER_MEMORY=1g# worker的工作通讯地址SPARK_WORKER_PORT=7078# worker的 webui地址SPARK_WORKER_WEBUI_PORT=8081## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot; 开启 hadoop 的 hdfs 和 yarn 集群 123start-dfs.shstart-yarn.sh 在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下: 123hadoop fs -mkdir /sparkloghadoop fs -chmod 777 /sparklog 将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置 1234567891011mv spark-defaults.conf.template spark-defaults.confvim spark-defaults.conf文末追加内容为：# 开启spark的日期记录功能spark.eventLog.enabled true# 设置spark日志记录的路径spark.eventLog.dir hdfs://node1:8020/sparklog/ # 设置spark日志是否启动压缩spark.eventLog.compress true 配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为 log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息） 123mv log4j.properties.template log4j.propertiesvim log4j.properties 12345结果显示：...18 # Set everything to be logged to the console19 log4j.rootCategory=WARN, console.... node1 节点分发 spark 安装文件夹 到 node2 和 node3上 12345cd /export/server/scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWDscp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD 在node2 和 node3 上做软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 重新加载环境变量 1source /etc/profile 进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh 123cd /export/server/spark/sbin ./start-history-server.sh 访问 WebUI 界面 123浏览器访问：http://node1:18080/ 启动Spark的Master和Worker进程 1234567891011121314151617# 启动全部master和workersbin/start-all.sh# 或者可以一个个启动:# 启动当前机器的mastersbin/start-master.sh# 启动当前机器的workersbin/start-worker.sh# 停止全部sbin/stop-all.sh# 停止当前机器的mastersbin/stop-master.sh# 停止当前机器的workersbin/stop-worker.sh 访问 WebUI界面 123浏览器访问：http://node1:8080/","categories":[],"tags":[]},{"title":"Spark基础配置","slug":"Spark基础配置","date":"2022-05-20T07:43:12.000Z","updated":"2022-05-28T08:54:47.707Z","comments":true,"path":"2022/05/20/Spark基础配置/","link":"","permalink":"http://dixiaoli.github.io/2022/05/20/Spark%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/","excerpt":"","text":"一、安装配置 jdk 编译环境软件安装目录 1mkdir -p /export/server JDK 1.8安装 上传 jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 1tar -zxvf jdk-8u241-linux-x64.tar.gz 配置环境变量 12345vim /etc/profileexport JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 重新加载环境变量文件 1source /etc/profile 查看 java 版本号 1java -version 12345结果显示：[root@node1 jdk1.8.0_241]# java -versionjava version &quot;1.8.0_241&quot;Java(TM) SE Runtime Environment (build 1.8.0_241-b07)Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) node1 节点将 java 传输到 node2 和 node3 12scp -r /export/server/jdk1.8.0_241/ root@node2:/export/server/scp -r /export/server/jdk1.8.0_241/ root@node3:/export/server/ 配置 node1和 node3的 jdk 环境变量（注：和上方 node1 的配置方法一样） 在 node1、node2 和node3 创建软连接 123cd /export/serverln -s jdk1.8.0_241/ jdk 重新加载环境变量文件 1source /etc/profile 二、zookeeper安装配置 配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 node1.root node2.root node3.root 123456789vim /etc/hosts#结果显示127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.88.135 node1 node1.root192.168.88.136 node2 node2.root192.168.88.137 node3 node3.root zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 123cd /export/server/tar -zxvf zookeeper-3.4.10.tar.gz 在 &#x2F;export&#x2F;server 目录下创建软连接 123cd /export/serverln -s zookeeper-3.4.10/ zookeeper 进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 123cd /export/server/zookeeper/conf/ cp zoo_sample.cfg zoo.cfg 接上步给 zoo.cfg 添加内容 1234567891011121314#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去 12345cd /export/server/zookeeper/zkdatatouch myidecho &#x27;1&#x27; &gt; myid 将 node1节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给node2 和 node3 123scp -r /export/server/zookeeper-3.4.10/ node2:$PWDscp -r /export/server/zookeeper-3.4.10/ node3:$PWD 推送成功后，分别在 node2 和 node3 上创建软连接 1ln -s zookeeper-3.4.10/ zookeeper 接上步推送完成后将 node2 和 node3 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid 中的内容分别改为 2 和 3 123456789cd /export/server/zookeeper/zkdatas/结果显示：[root@node2 zkdatas]# vim myid [root@node2 zkdatas]# more myid 2[root@node3 zkdatas]# vim myid [root@node3 zkdatas]# more myid 3 配置zookeeper的环境变量（注：三台主机都需要配置） 12345vim /etc/profile# zookeeper 环境变量export ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin 重新加载环境变量文件 1source /etc/profile 进入 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做） 123cd /export/server/zookeeper-3.4.10/bin zkServer.sh start 12345结果显示：[root@node1 bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 查看 zookeeper 的状态 1zkServer.sh status 123456789101112131415结果显示：[root@node1 server]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[root@node2 server]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[root@node3 conf]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader 1jps 123456789101112结果显示：[root@node1 server]# jps125348 QuorumPeerMain16311 Jps[root@node2 server]# jps126688 QuorumPeerMain17685 Jps[root@node3 conf]# jps126733 QuorumPeerMain17727 Jps 脚本一键启动 1234567891011121314151617181920212223242526272829303132333435vim zkServer.sh#!/bin/bashif [ $# -eq 0 ] ;then echo &quot;please input param:start stop&quot;elseif [ $1 = start ] ;then echo &quot;$&#123;1&#125;ing node1&quot; ssh master &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot; for i in &#123;2..3&#125; do echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot; ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot; donefiif [ $1 = stop ];then echo &quot;$&#123;1&#125;ping node1 &quot; ssh node1 &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot; for i in &#123;2..3&#125; do echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot; ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot; donefiif [ $1 = status ];then echo &quot;$&#123;1&#125;ing node1&quot; ssh node1 &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot; for i in &#123;2..3&#125; do echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot; ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot; donefifi 12# 将文件放在 /bin 目录下chmod +x zkServer-all.sh &amp;&amp; zkServer-all.sh 三、Hadoop 安装配置 把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 &#x2F;export&#x2F;server 并解压文件 1tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 修改配置文件(进入路径 &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop) 1cd /export/server/hadoop-3.3.0/etc/hadoop hadoop-env.sh 12345678#文件最后添加export JAVA_HOME=/export/server/jdk1.8.0_241export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root core-site.xml 12345678910111213141516171819202122232425262728293031323334&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; hdfs-site.xml 12345&lt;!-- 设置SNN进程运行机器位置信息 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt; mapred-site.xml 1234567891011121314151617181920212223242526272829303132&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt; yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; workers 123node1node2node3 分发同步hadoop安装包 1234cd /export/serverscp -r hadoop-3.3.0 root@node2:$PWDscp -r hadoop-3.3.0 root@node3:$PWD 将hadoop添加到环境变量 1234vim /etc/profileexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 重新加载环境变量文件 1source /etc/profile Hadoop集群启动 格式化namenode（只有首次启动需要格式化） 1hdfs namenode -format 脚本一键启动 12345678910111213[root@node1 ~]# start-dfs.sh Starting namenodes on [node1]上一次登录：五 3月 11 21:27:24 CST 2022pts/0 上Starting datanodes上一次登录：五 3月 11 21:27:32 CST 2022pts/0 上Starting secondary namenodes [node2]上一次登录：五 3月 11 21:27:35 CST 2022pts/0 上[root@node1 ~]# start-yarn.sh Starting resourcemanager上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上Starting nodemanagers上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上 启动后 输入 jps 查看 1234567891011121314151617[root@node1 ~]# jps127729 NameNode127937 DataNode14105 Jps128812 NodeManager128591 ResourceManager[root@node2 hadoop]# jps121889 NodeManager121559 SecondaryNameNode7014 Jps121369 DataNode[root@node3 hadoop]# jps6673 Jps121543 NodeManager121098 DataNode WEB页面 HDFS集群： 1http://node1:9870/ YARN集群： 1http://node1:8088/","categories":[],"tags":[]}],"categories":[],"tags":[]}